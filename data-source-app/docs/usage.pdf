# PostgreSQL Metadata App - Usage Guide

## Overview
This guide covers how to use the PostgreSQL Metadata App to extract metadata and quality metrics from your PostgreSQL databases. The application provides both command-line interface and programmatic access to database metadata.

## Basic Commands

### Metadata Extraction

#### Scan Specific Schema
```bash
python -m src.app scan --schema public --format json
```

**Options:**
- `--schema, -s`: Target schema name
- `--format, -f`: Output format (json, csv, both)
- `--config, -c`: Configuration file path
- `--verbose, -v`: Enable detailed logging

#### Scan All Schemas
```bash
python -m src.app scan-all --format both
```

This command scans all available schemas in the database.

#### Quality Metrics Analysis
```bash
python -m src.app quality-metrics --schema ecommerce --format csv
```

**Options:**
- `--schema, -s`: Target schema for analysis
- `--format, -f`: Output format (json, csv, both)
- `--config, -c`: Configuration file path
- `--verbose, -v`: Enable detailed logging

### Sample Data Management

#### Load Sample Data
```bash
python -m src.app populate-sample
```

This loads the included sample e-commerce database for testing.

## Advanced Usage

### Configuration Management

#### Using Custom Config File
```bash
python -m src.app scan --config my_config.yml --schema public
```

#### Environment Variables
```bash
export POSTGRES_DSN="postgresql://user:pass@host:port/db"
python -m src.app scan-all
```

#### Verbose Logging
```bash
python -m src.app scan --verbose --log-file debug.log
```

### Output Management

#### JSON Export
```bash
python -m src.app scan --format json --schema public
```

**Output Location:** `./output/json/metadata_YYYYMMDD_HHMMSS.json`

**Structure:**
```json
{
  "export_info": {
    "timestamp": "2024-01-15T10:30:00",
    "version": "1.0.0",
    "total_schemas": 1
  },
  "schemas": [
    {
      "name": "public",
      "owner": "postgres",
      "tables": [...]
    }
  ]
}
```

#### CSV Export
```bash
python -m src.app scan --format csv --schema public
```

**Output Files:**
- `metadata_schemas.csv` - Schema information
- `metadata_tables.csv` - Table details
- `metadata_columns.csv` - Column information
- `metadata_constraints.csv` - Constraint details
- `metadata_indexes.csv` - Index information

#### Combined Export
```bash
python -m src.app scan --format both --schema public
```

This creates both JSON and CSV exports simultaneously.

## Metadata Types

### Schema Metadata
- **Schema Name**: Name of the database schema
- **Owner**: Schema owner (usually postgres)
- **Table Count**: Number of tables in schema

### Table Metadata
- **Table Name**: Name of the table
- **Table Type**: BASE TABLE, VIEW, etc.
- **Comment**: Table description from PostgreSQL comments
- **Tags**: Parsed tags from comments
- **Column Count**: Number of columns
- **Constraint Count**: Number of constraints
- **Index Count**: Number of indexes

### Column Metadata
- **Column Name**: Name of the column
- **Position**: Ordinal position in table
- **Data Type**: PostgreSQL data type
- **Is Nullable**: Whether column allows NULL values
- **Default Value**: Default value if specified
- **Max Length**: Maximum character length
- **Precision**: Numeric precision
- **Scale**: Numeric scale
- **Comment**: Column description
- **Tags**: Parsed tags from comments

### Constraint Metadata
- **Constraint Name**: Name of the constraint
- **Constraint Type**: PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK
- **Columns**: Columns involved in constraint
- **Referenced Table**: For foreign keys
- **Referenced Schema**: For foreign keys
- **Referenced Columns**: For foreign keys

### Index Metadata
- **Index Name**: Name of the index
- **Definition**: SQL definition
- **Columns**: Columns in index
- **Is Unique**: Whether index enforces uniqueness
- **Is Primary**: Whether index is primary key

## Quality Metrics

### Column-level Metrics
- **Total Count**: Number of rows analyzed
- **Non-null Count**: Number of non-null values
- **Null Count**: Number of null values
- **Null Percentage**: Percentage of null values
- **Distinct Count**: Number of unique values
- **Distinct Percentage**: Percentage of unique values
- **Top Values**: Most frequent values with counts

### Table-level Metrics
- **Row Count**: Total number of rows
- **Column Count**: Number of columns
- **High Null Columns**: Columns with >50% null values
- **Low Distinct Columns**: Columns with <10% distinct values

### Quality Scoring
- **Overall Score**: 0-100 based on data quality
- **High Null Penalty**: -30 points for high null columns
- **Low Distinct Penalty**: -20 points for low distinct columns

## Business Context

### Comment-based Tags
Parse tags from PostgreSQL comments:
```sql
COMMENT ON TABLE customers IS 'Customer data [tags: master-data,user]';
COMMENT ON COLUMN customers.email IS 'Email address [tags: pii,contact]';
```

### YAML Metadata
Additional business context via YAML files:
```yaml
ecommerce:
  customers:
    description: "Customer master data"
    tags: ["master-data", "customer"]
    columns:
      email:
        description: "Primary contact email"
        tags: ["pii", "contact", "unique"]
```

## Output Analysis

### JSON Output Analysis
```python
import json

# Load metadata
with open('output/json/metadata_20240115_103000.json', 'r') as f:
    metadata = json.load(f)

# Analyze schemas
for schema in metadata['schemas']:
    print(f"Schema: {schema['name']}")
    print(f"Tables: {len(schema['tables'])}")
    
    for table in schema['tables']:
        print(f"  Table: {table['name']}")
        print(f"  Columns: {len(table['columns'])}")
        print(f"  Tags: {table['tags']}")
```

### CSV Output Analysis
```python
import pandas as pd

# Load table metadata
tables_df = pd.read_csv('output/csv/metadata_tables.csv')
print(tables_df.head())

# Analyze column types
columns_df = pd.read_csv('output/csv/metadata_columns.csv')
type_counts = columns_df['data_type'].value_counts()
print(type_counts)
```

## Automation and Scheduling

### Cron Job (Linux/macOS)
```bash
# Daily metadata extraction at 2 AM
0 2 * * * cd /path/to/app && python -m src.app scan-all --format both
```

### Windows Task Scheduler
1. Create basic task
2. Set trigger (daily at 2 AM)
3. Set action: Start a program
4. Program: `python`
5. Arguments: `-m src.app scan-all --format both`
6. Start in: `/path/to/app`

### Docker Container
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["python", "-m", "src.app", "scan-all"]
```

## Integration Examples

### Data Catalog Integration
```python
from src.extractor.metadata_extractor import MetadataExtractor
from src.db.connection import DatabaseConnection
from src.config import AppConfig

# Load configuration
config = AppConfig.from_file('config.yml')

# Connect to database
db_conn = DatabaseConnection(config.database.get_connection_string())

# Extract metadata
extractor = MetadataExtractor(db_conn, config)
schemas = extractor.extract_all_metadata(['public'])

# Process for data catalog
for schema in schemas:
    for table in schema.tables:
        # Add to data catalog
        catalog.add_table(
            name=table.name,
            schema=table.schema,
            description=table.comment,
            tags=table.tags
        )
```

### Quality Monitoring
```python
from src.extractor.quality_metrics import QualityMetricsExtractor

# Extract quality metrics
metrics_extractor = QualityMetricsExtractor(db_conn, config)
metrics = metrics_extractor.extract_all_metrics(['public'])

# Monitor quality trends
for schema_name, table_metrics in metrics.items():
    for table in table_metrics:
        if table.row_count > 1000:  # Large table
            for col in table.column_metrics:
                if col.null_percentage > 50:
                    # Alert on high null percentage
                    send_alert(f"High null percentage in {table.table_name}.{col.column_name}")
```

## Best Practices

### Configuration Management
1. Use environment variables for sensitive data
2. Keep configuration files in version control
3. Use separate configs for different environments
4. Document configuration changes

### Performance Optimization
1. Use sampling for large tables
2. Process schemas individually for large databases
3. Schedule runs during off-peak hours
4. Monitor memory usage

### Data Governance
1. Use consistent tagging conventions
2. Maintain YAML metadata files
3. Regular metadata extraction
4. Quality metrics monitoring

### Security
1. Use read-only database users
2. Secure configuration files
3. Encrypt sensitive data
4. Regular security updates

## Troubleshooting

### Common Issues
1. **Connection Timeout**: Check network and database status
2. **Permission Denied**: Verify database user permissions
3. **Memory Error**: Reduce sample limits or process smaller batches
4. **File Not Found**: Check file paths and permissions

### Debug Mode
```bash
python -m src.app scan --verbose --log-file debug.log
```

### Performance Profiling
```bash
python -c "
import cProfile
import pstats
from src.app import main

cProfile.run('main()', 'profile.stats')
p = pstats.Stats('profile.stats')
p.sort_stats('cumulative').print_stats(10)
"
```

## Support and Resources

- **Documentation**: README.md and setup guide
- **Sample Data**: Use `populate-sample` command
- **GitHub Issues**: Report bugs and request features
- **Community**: Join discussions and share experiences

